{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix Math\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Transformations can be rotations, reflections, scaling, shears, and flattenings.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "One major theme in the study of matrix algebra is that of matrix decompositions. The idea is to take a matrix $A$\n",
    " (either arbitrary or satisfying some reasonable conditions) and decompose it as the product of two or three matrices of a simpler form. For example, the so-called QR decomposition writes an arbitrary matrix $X$\n",
    " as\n",
    "$$\n",
    "X=QR\n",
    "$$\n",
    "\n",
    "Where $Q$ is orthogonal (a composition of rotations + reflections) and $R$ is upper-triangular. In two-dimensions, $Q$ is either rotation or reflection.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In the subject of SVD, an important matrix decomposition is the aptly-named singular value decomposition. This writes an arbitrary matrix $X$ in the form\n",
    "$$\n",
    "X = U \\Sigma V^T\n",
    "$$\n",
    "where $\\Sigma$ is a nonnegative, diagonal matrix whose entries are called the singular values, $U$\n",
    " and $V$  are orthogonal (unitary in the complex case), and $V^T$ is the transpose of $V$\n",
    " (or conjugate-transpose if complex entries are allowed). A diagonal matrix can be thought of as stretching a basis by the corresponding singular values, and an orthogonal matrix is a rotation or reflection. Thus, this decomposition sees $T$\n",
    " as a composition of a sort of rotation (transposed), a stretching, and finally another rotation.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Properties of Rotation Matrix\n",
    "\n",
    "There are certain properties that are applicable to both 2D and 3D rotation matrices. These are as follows:\n",
    "\n",
    "1. A rotation matrix will always be a square matrix.\n",
    "2. As a rotation matrix is always an orthogonal matrix the transpose will be equal to the inverse of the matrix.\n",
    "3. The determinant of a rotation matrix will always be equal to 1.\n",
    "4. Multiplication of rotation matrices will result in a rotation matrix.\n",
    "5. If we take the cross product of two rows of a rotation matrix it will be equal to the third.\n",
    "6. The dot product of a row with a column of a rotation matrix will be equal to 1.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Selection Matrix\n",
    "\n",
    "In the simplest form, left-multiplying a matrix $M$ by the $i^\\text{th}$ row of the identity matrix picks out the $i^\\text{th}$ row of $M$; right-multiplying by the $j^{th}$ column of the identity picks out the $j^{th}$ column of $M$.\n",
    "\n",
    "Let $e _j$  be an $m \\times 1$ vector of zeros with a one in $j^{th}$ position. Then $m_j = M  e_j$ would be the $j^{th}$ column of $M$.\n",
    "\n",
    "For example, if, from matrix:\n",
    "$$\n",
    "M =\n",
    "\\begin{pmatrix}\n",
    "a & b & c & d\\\\\n",
    "e & f & g & h\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "You wanted to generate the submatrix:\n",
    "\n",
    "$$\n",
    "M_2 =\n",
    "\\begin{pmatrix}\n",
    "a & c  & d\\\\\n",
    "e & g & h\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "You could multiply by the appropriate matrix on the right:\n",
    "$$\n",
    "M2 = M\n",
    "\\begin{pmatrix}\n",
    "1 & 0 & 0 \\\\\n",
    "0 & 0 & 0 \\\\\n",
    "0 & 1 & 0 \\\\\n",
    "0 & 0 & 1\n",
    "\\end{pmatrix}\n",
    "$$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Principal Component Analysis\n",
    "\n",
    "Principal component analysis, or PCA, is a dimensionality-reduction method that is often used to reduce the dimensionality of large data sets, by transforming a large set of variables into a smaller one that still contains most of the information in the large set.\n",
    "\n",
    "Somewhat unsurprisingly, reducing the dimension of the feature space is called “dimensionality reduction.” There are many ways to achieve dimensionality reduction, but most of these techniques fall into one of two classes:\n",
    "\n",
    "**Feature elimination** is what it sounds like: we reduce the feature space by eliminating features. Advantages of feature elimination methods include simplicity and maintaining interpretability of your variables. By eliminating features, we’ve also entirely eliminated any benefits those dropped variables would bring. As a disadvantage, though, you gain no information from those variables you’ve dropped.\n",
    "\n",
    "Reducing the number of variables of a data set naturally comes at the expense of accuracy, but the trick in dimensionality reduction is to trade a little accuracy for simplicity. Because smaller data sets are easier to explore and visualize and make analyzing data much easier and faster for algorithms without extraneous variables to process.\n",
    "\n",
    "**Feature extraction**, however, doesn’t run into this problem. Say we have ten independent variables. In feature extraction, we create ten “new” independent variables, where each “new” independent variable is a combination of each of the ten “old” independent variables. However, we create these new independent variables in a specific way and order these new variables by how well they predict our dependent variable. We keep as many of the new independent variables as we want, but we drop the “least important ones.” Because we ordered the new variables by how well they predict our dependent variable, we know which variable is the most important and least important. But — and here’s the kicker — because these new independent variables are combinations of our old ones, we’re still keeping the most valuable parts of our old variables, even when we drop one or more of these “new” variables! As an added benefit, each of the “new” variables after PCA are all independent of one another.\n",
    "\n",
    "So, to sum up, the idea of PCA is simple — reduce the number of variables of a data set, while preserving as much information as possible.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### When do you use PCA?\n",
    "\n",
    "1. Do you want to reduce the number of variables, but aren’t able to identify variables to completely remove from consideration?\n",
    "2. Do you want to ensure your variables are independent of one another?\n",
    "3. Are you comfortable making your independent variables less interpretable?\n",
    "\n",
    "If you answered “yes” to all three questions, then PCA is a good method to use. If you answered “no” to question 3, you **should not** use PCA.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### How you do principal component analysis in five steps\n",
    "\n",
    "1. Standardize the range of continuous initial variables.\n",
    "2. Compute the covariance matrix to identify correlations.\n",
    "3. Compute the eigenvectors and eigenvalues of the covariance matrix to identify the principal components.\n",
    "4. Create a feature vector to decide which principal components to keep.\n",
    "5. Recast the data along the principal components axes.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 1. Standardization\n",
    "\n",
    "We standardize the data because PCA is quite sensitive to the magnitude and variance of the data. A variable that has the range 0 to 1,000 will dominate a variable with a range of 0 to 1 and lead to biased results. To standardize, we simply subtract the mean and divide by the standard deviation:\n",
    "$$\n",
    "z_i = \\frac {x_i - \\bar{x}}{\\sigma_x}\n",
    "$$\n",
    "After standardization, all the data will be the same scale.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 2. Compute the Covariance\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 3. Compute the eigenvectors and eigenvalues to determine the principal components\n",
    "\n",
    "Principal components are new variables that are constructed as linear combinations or mixtures of the initial variables. These combinations are done in such a way that the new variables (i.e., principal components) are uncorrelated and most of the information within the initial variables is squeezed or compressed into the first components. So, the idea is 10-dimensional data gives you 10 principal components, but PCA tries to put maximum possible information in the first component, then maximum remaining information in the second and so on.\n",
    "Organizing information in principal components this way, will allow you to reduce dimensionality without losing much information, and this by discarding the components with low information and considering the remaining components as your new variables.\n",
    "\n",
    "An important thing to realize here is that the principal components are less interpretable and don’t have any real meaning since they are constructed as linear combinations of the initial variables.\n",
    "\n",
    "Geometrically speaking, principal components represent the directions of the data that explain a maximal amount of variance, that is to say, the lines that capture most information of the data. The relationship between variance and information here, is that, the larger the variance carried by a line, the larger the dispersion of the data points along it, and the larger the dispersion along a line, the more information it has. To put all this simply, just think of principal components as new axes that provide the best angle to see and evaluate the data, so that the differences between the observations are better visible.\n",
    "\n",
    "Eigenvectors and eigenvalues who are behind all the magic explained above, because the eigenvectors of the Covariance matrix are actually the directions of the axes where there is the most variance(most information) and that we call Principal Components. And eigenvalues are simply the coefficients attached to eigenvectors, which give the amount of variance carried in each Principal Component. One way to think of a real eigenvalue is the amount by which a matrix stretches or shrinks things along a certain axis—the associated eigenvector.\n",
    "\n",
    "By ranking your eigenvectors in order of their eigenvalues, highest to lowest, you get the principal components in order of significance.\n",
    "\n",
    "After having the principal components, to compute the percentage of variance (information) accounted for by each component, we divide the eigenvalue of each component by the sum of eigenvalues.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 4. Feature Selection\n",
    "\n",
    "Computing the eigenvectors and ordering them by their eigenvalues in descending order, allows us to find the principal components in order of significance. In this step, what we do is, to choose whether to keep all these components or discard those of lesser significance (of low eigenvalues), and form with the remaining ones a matrix of vectors that we call Feature vector.\n",
    "\n",
    "There are three methods to choose the cutoff for feature selection:\n",
    "\n",
    "1. Arbitrarily select a number of principal components to include.\n",
    "2. Include enough principal components to explain X% of the total variability.\n",
    "3. Find the \"knee\". Using a scree plot, look for a big drop between eigenvalues.\n",
    "\n",
    "So, the feature vector is simply a matrix that has as columns the eigenvectors of the components that we decide to keep. This makes it the first step towards dimensionality reduction, because if we choose to keep only p eigenvectors (components) out of n, the final data set will have only p dimensions.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 5. Recast the data\n",
    "\n",
    "In this step, which is the last one, the aim is to use the feature vector formed using the eigenvectors of the covariance matrix, to reorient the data from the original axes to the ones represented by the principal components (hence the name Principal Components Analysis). This can be done by multiplying the transpose of the original data set by the transpose of the feature vector.\n",
    "$$\n",
    "X^* =  P^T Z^T\n",
    "$$\n",
    "The asterisk (*) here denotes the transformed data.\n",
    "\n",
    "**NB:** This is an optional step depending on the analysis being performed."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Singular Value Decomposition"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVD is a numerically stable matrix decomposition that is guaranteed to exist for any matrix (unlike eigen-decomposition). SVD generalizes the concept of the fast Fourier transform (FFT) in that it is a more generic, data-driven technique. SVD provides a basis that is **tailored** to the specific data, whereas FFT provides a **generic** basis. SVD provides a systematic way to determine a low-dimensional approximation of high-dimensional data in terms of dominant correlations.\n",
    "\n",
    "There are many powerful applications of SVD beyond dimensionality reduction, including the computation of the pseudo-inverse of non-square matrices, providing the solution to under-determined or over-determined systems of equations, and de-noising data sets."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While in this course we are only interested in real-valued matrices, the SVD is a unique matrix decomposition for any complex-valued matrix $X \\in \\mathbf{C}^{m \\times n}$ (henceforth, we will restrict ourselves to $\\mathbf{R}$, but you should know that SVD is more general).\n",
    "$$X = U \\Sigma V^T$$\n",
    "Where $U \\in \\mathbf{R}^{m \\times m}$ and $V \\in \\mathbf{R}^{n \\times n}$ are **unitary** matrices with orthogonal columns, and $\\Sigma \\in \\mathbf{R}^{m \\times n}$ is a matrix with non-negative entries on the diagonal and zeros off diagonal.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When $m \\ge n$, the matrix $\\Sigma$ has **at most** $m$ non-zero elements and can be written:\n",
    "$$\n",
    "\\Sigma =\n",
    "\\begin{bmatrix}\n",
    "\\hat\\Sigma \\\\\n",
    "0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Therefore, it is possible to **exactly** represent $X$ using the **economy** SVD:\n",
    "$$\n",
    "X = U\\Sigma V^T =\n",
    "\\begin{bmatrix}\n",
    "\\hat U & \\hat U^\\perp\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "\\hat\\Sigma \\\\\n",
    "0\n",
    "\\end{bmatrix}\n",
    "V^T\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"SVD.png\">\n",
    "</center>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns of $U^\\perp$ span a vector space that is complementary and orthogonal to that spanned by $\\hat U$. The columns of $U$ are called the left singular vectors of $X$ and the columns of $V$ are the right singular vectors. The diagonal elements of $\\hat \\Sigma$ are called the singular values and they are ordered from the largest to the smallest. The rank of $X$ is equal to the number of non-zero singular values."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the SVD\n",
    "Although you will generally use standard numerical implementations to compute the SVD, the process roughly follows a few steps:\n",
    "* First, reduce the matrix $X$ to a bi-diagonal matrix; then use an iterative algorithm to compute the SVD of the bi-diagonal matrix.\n",
    "* If the matrix has $m \\gg n$ then the first step is achieved by first computing the QR factorization to reduce $X$ to an upper triangular matrix; followed by Householder reflections to reduce the upper triangular matric to a bi-diagonal form.\n",
    "\n",
    "In Python, this can be computed as in this code snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X = np.random.rand(11,5) # create sample data matrix\n",
    "U, S, VT = np.linalg.svd(X, full_matrices=True) # this is full SVD\n",
    "U_hat, S_hat, VT_hat = np.linalg.svd(X, full_matrices=False) # this is economy SVD"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Low-rank approximations using SVD\n",
    "\n",
    "SVD provides a hierarchy of low-rank approximations for the matrix $X$.\n",
    "\n",
    "Since $\\Sigma$ is diagonal, an equivalent way to write an SVD is as the sum of rank-1 matrices, each given by the product of the left singular vector and the right singular vector weighted by the singular value.\n",
    "$$\n",
    "X = U \\Sigma V^T = \\sigma_1 u_1 v_1^T + \\cdots + \\sigma_n u_n v_n^T\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "To get the best low-rank approximation to matrix $X$ (in a mean-squared error sense), simply truncate the expansion after $r$ terms:\n",
    "$$\n",
    "X_{(r)} = \\sigma_1 u_1 v_1 + \\cdots + \\sigma_r u_r v_R^T\n",
    "$$\n",
    "\n",
    "\n",
    "We have already discussed (in the context of PCA) how to select the truncation rank $r$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Frobenius Norm and error bounds of low-rank approximation\n",
    "\n",
    "The Frobenius norm of a matrix is the equivalent of the Euclidean norm for vectors. It is given by the square root of the sum of the squared elements of the matrix. Thus, for an $m \\times n$ matrix $X$ the Frobenius norm is given by:\n",
    "$$\n",
    "||X|| _F = \\sqrt{\\sum_{i=1}^m \\sum _{j=1}^n x_{ij}^2}\n",
    "$$\n",
    "This is the same value we would get if we reshaped $X$ into a vector and evaluated its vector norm.\n",
    "\n",
    "Given this definition, the optimal rank-r approximation given above minimizes the squared Frobenius norm $||X - X_{(r)}||_F^2$ which is the sum of squared errors between $X$ and its rank-r approximation.\n",
    "\n",
    "Although it may be more useful to consider the relative error because error generally scales with the size and magnitude of $X$:\n",
    "$$\n",
    "\\frac{||X - X_{(r)}||_F^2}{||X||_F^2}\n",
    "$$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Determinant\n",
    "\n",
    "The determinant of a square matrix quantifies how that matrix changes the volume of unit hypercube. The absolute value of the determinant of a square matrix is equal to the product of its singular values:\n",
    "$$\n",
    "| \\text{det} \\left( X\\right)| = \\prod_{i=1}^n s_{i}\n",
    "$$\n",
    "where $\\text{\\textbraceleft} \\sigma_i \\text{\\textbraceright}$ are the singular values of $X$.\n",
    "\n",
    "You can see this intuitively by thinking about the SVD of $X$ which consists of a rotation (by $V^T$), a stretching along the cardinal axes (by $\\sigma_i$ for each direction), and a second rotation (by $U$). The stretching by $\\Sigma$ is the only part of $X$ that increases or decreases volume.\n",
    "\n",
    "Another definition of the determinant is that it is equal to the product of the eigenvalues of a matrix:\n",
    "$$\n",
    "\\text{det} \\left( X\\right) = \\prod_{i=1}^n e_{i}\n",
    "$$\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3e6b2df9b109e3c6fb59c52c4b7d0de50c5b5ffa8dffcba1947487826ddefa10"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
