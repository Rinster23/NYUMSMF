{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41d8bc0a",
   "metadata": {},
   "source": [
    "# Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "2b670865",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>housing_median_age</th>\n",
       "      <th>total_rooms</th>\n",
       "      <th>total_bedrooms</th>\n",
       "      <th>population</th>\n",
       "      <th>households</th>\n",
       "      <th>median_income</th>\n",
       "      <th>median_house_value</th>\n",
       "      <th>ocean_proximity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-122.23</td>\n",
       "      <td>37.88</td>\n",
       "      <td>41.0</td>\n",
       "      <td>880.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>322.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>8.3252</td>\n",
       "      <td>452600.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-122.22</td>\n",
       "      <td>37.86</td>\n",
       "      <td>21.0</td>\n",
       "      <td>7099.0</td>\n",
       "      <td>1106.0</td>\n",
       "      <td>2401.0</td>\n",
       "      <td>1138.0</td>\n",
       "      <td>8.3014</td>\n",
       "      <td>358500.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-122.24</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1467.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>496.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>7.2574</td>\n",
       "      <td>352100.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-122.25</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1274.0</td>\n",
       "      <td>235.0</td>\n",
       "      <td>558.0</td>\n",
       "      <td>219.0</td>\n",
       "      <td>5.6431</td>\n",
       "      <td>341300.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-122.25</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1627.0</td>\n",
       "      <td>280.0</td>\n",
       "      <td>565.0</td>\n",
       "      <td>259.0</td>\n",
       "      <td>3.8462</td>\n",
       "      <td>342200.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n",
       "0    -122.23     37.88                41.0        880.0           129.0   \n",
       "1    -122.22     37.86                21.0       7099.0          1106.0   \n",
       "2    -122.24     37.85                52.0       1467.0           190.0   \n",
       "3    -122.25     37.85                52.0       1274.0           235.0   \n",
       "4    -122.25     37.85                52.0       1627.0           280.0   \n",
       "\n",
       "   population  households  median_income  median_house_value ocean_proximity  \n",
       "0       322.0       126.0         8.3252            452600.0        NEAR BAY  \n",
       "1      2401.0      1138.0         8.3014            358500.0        NEAR BAY  \n",
       "2       496.0       177.0         7.2574            352100.0        NEAR BAY  \n",
       "3       558.0       219.0         5.6431            341300.0        NEAR BAY  \n",
       "4       565.0       259.0         3.8462            342200.0        NEAR BAY  "
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import tarfile\n",
    "from six.moves import urllib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "DOWNLOAD_ROOT = \"https://raw.githubusercontent.com/ageron/handson-ml/master/\"\n",
    "HOUSING_PATH = os.path.join(\"datasets\", \"housing\")\n",
    "HOUSING_URL = DOWNLOAD_ROOT + \"datasets/housing/housing.tgz\"\n",
    "\n",
    "def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):\n",
    "    \n",
    "    if not os.path.isdir(housing_path):\n",
    "        os.makedirs(housing_path)\n",
    "        \n",
    "    tgz_path = os.path.join(housing_path, \"housing.tgz\")\n",
    "    if not os.path.isfile(tgz_path): #download data if not already there\n",
    "        urllib.request.urlretrieve(housing_url, tgz_path)\n",
    "        \n",
    "    housing_tgz = tarfile.open(tgz_path)\n",
    "    housing_tgz.extractall(path=housing_path)\n",
    "    housing_tgz.close()\n",
    "\n",
    "fetch_housing_data()\n",
    "\n",
    "def load_housing_data(housing_path=HOUSING_PATH):\n",
    "    csv_path = os.path.join(housing_path, \"housing.csv\")\n",
    "    return pd.read_csv(csv_path)\n",
    "\n",
    "housing = load_housing_data()\n",
    "housing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "99ef5661",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "# deal with skewness\n",
    "# Divide by 1.5 to limit the number of income categories\n",
    "housing[\"income_cat\"] = np.ceil(housing[\"median_income\"] / 1.5)\n",
    "# Label those above 5 as 5\n",
    "housing[\"income_cat\"].where(housing[\"income_cat\"] < 5, 5.0, inplace=True)\n",
    "\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "for train_index, test_index in split.split(housing, housing[\"income_cat\"]):\n",
    "    strat_train_set = housing.loc[train_index]\n",
    "    strat_test_set = housing.loc[test_index]\n",
    "for set_ in (strat_train_set, strat_test_set):\n",
    "    set_.drop(\"income_cat\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b00cd2c",
   "metadata": {},
   "source": [
    "    - Estimators: any object estimating some parameters. In the above, the imputer is an estimator. Estimators need to have a fit() method which take the dataset as input. Any other parameters are considered as hyperparameters, e.g. the strategy hyperparameter in the imputer\n",
    "\n",
    "    - Transformers: these are estimators which can transofrm the dataset. They need to implement the transform() method. All transformers also has a fit_transform() method equivalent to calling fit() and then transform(). Sometimes the fit_transform() method is better optimized for efficiency so usually best to call it instead of fit() and then transform(). The imputer above is actually a transformer.\n",
    "\n",
    "    - Predictors: these are estimators which can make predictions. LinearRegression model is a predictor. Predictors must implement a predict() method. They also have a score() method that measures the quality of the predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "8b6c01a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(158, 9)"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data preprocessing\n",
    "housing = strat_train_set.drop(\"median_house_value\", axis=1) # drop labels for training set\n",
    "housing_labels = strat_train_set[\"median_house_value\"].copy()\n",
    "# there are na values\n",
    "incomplete_rows = housing[housing.isnull().any(axis=1)] # Take out those with na value\n",
    "incomplete_rows.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab79d6c1",
   "metadata": {},
   "source": [
    "## Deal with NA values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "ce12f83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "# replace na by median\n",
    "housing_num = housing.drop(\"ocean_proximity\", axis=1) # median only works with numerical attributes\n",
    "# the followings are commented since later we use pipeline to solve na problems all together\n",
    "# imputer = SimpleImputer(strategy=\"median\")\n",
    "# imputer.fit(housing_num)\n",
    "# print(imputer.statistics_)\n",
    "# print(housing_num.median().values)\n",
    "# X = imputer.transform(housing_num)\n",
    "# housing_tr = pd.DataFrame(X, columns=housing_num.columns, index = list(housing.index.values))\n",
    "# # show that na values has been replaced\n",
    "# housing_tr.loc[sample_incomplete_rows.index.values]\n",
    "# # back to index starting form 1\n",
    "# housing_tr = pd.DataFrame(X, columns=housing_num.columns)\n",
    "# housing_tr.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9cc674",
   "metadata": {},
   "source": [
    "## Deal with the categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "73e4b15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "housing_cat = housing[\"ocean_proximity\"]\n",
    "housing_cat_encoded = encoder.fit_transform(housing_cat)\n",
    "# print(housing_cat_encoded)\n",
    "# print(encoder.classes_) ['<1H OCEAN' 'INLAND' 'ISLAND' 'NEAR BAY' 'NEAR OCEAN']\n",
    "# However the LabelEncoder above assumes order. E.g. label 0 is < than label 4. This should not be the case for\n",
    "# the encoder classes for the housing dataset. That is why we use instead a one-hot-encoder\n",
    "\n",
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "# encoder = OneHotEncoder(categories='auto')\n",
    "# housing_cat_1hot = encoder.fit_transform(housing_cat_encoded.reshape(-1,1))\n",
    "# housing_cat_1hot # note that the output is a sparse matrix\n",
    "# housing_cat_1hot.toarray() can convert back to numpy array... but this takes lots of memory!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1f9d1d",
   "metadata": {},
   "source": [
    "## Use pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "6d24315e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "# column index\n",
    "rooms_ix, bedrooms_ix, population_ix, household_ix = 3, 4, 5, 6\n",
    "# add some features \n",
    "class CombinedAttributesAdder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, add_bedrooms_per_room = True): # no *args or **kargs\n",
    "        self.add_bedrooms_per_room = add_bedrooms_per_room\n",
    "    def fit(self, X, y=None):\n",
    "        return self  # nothing else to do\n",
    "    def transform(self, X, y=None):\n",
    "        rooms_per_household = X[:, rooms_ix] / X[:, household_ix]\n",
    "        population_per_household = X[:, population_ix] / X[:, household_ix]\n",
    "        if self.add_bedrooms_per_room:\n",
    "            bedrooms_per_room = X[:, bedrooms_ix] / X[:, rooms_ix]\n",
    "            return np.c_[X, rooms_per_household, population_per_household,\n",
    "                         bedrooms_per_room]\n",
    "        else:\n",
    "            return np.c_[X, rooms_per_household, population_per_household]\n",
    "\n",
    "# attr_adder = CombinedAttributesAdder(add_bedrooms_per_room=False)\n",
    "# housing_extra_attribs = attr_adder.transform(housing.values)\n",
    "# housing_extra_attribs = pd.DataFrame(housing_extra_attribs, \n",
    "#                                      columns=list(housing.columns) + [\"rooms_per_household\", \"population_per_household\"])\n",
    "# housing_extra_attribs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbd9cf4a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'housing_cat_encoded' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_1284\\268786207.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[0mppflb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPipelineFriendlyLabelBinarizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[0mhousing_cat_1hot_lb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mppflb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhousing_cat_encoded\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[0mhousing_cat_1hot_lb\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'housing_cat_encoded' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# Create a class to select numerical or categorical columns \n",
    "# since Scikit-Learn doesn't handle DataFrames yet\n",
    "class DataFrameSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, attribute_names):\n",
    "        self.attribute_names = attribute_names\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return X[self.attribute_names].values\n",
    "    \n",
    "# LabelBinarizer is same as OneHotEncoder but fit_transform() equivalent to fit_transform().to_array() of OneHotEncoder\n",
    "# unfortunately LabelBinarizer isn't pipeline friendly so we'll have to extend it as below:\n",
    "from sklearn.preprocessing import LabelBinarizer \n",
    "class PipelineFriendlyLabelBinarizer(LabelBinarizer):\n",
    "    def fit_transform(self, X, y=None):\n",
    "        return super(PipelineFriendlyLabelBinarizer, self).fit_transform(X)\n",
    "    \n",
    "ppflb = PipelineFriendlyLabelBinarizer()\n",
    "housing_cat_1hot_lb = ppflb.fit_transform(housing_cat_encoded.reshape(-1,1))\n",
    "housing_cat_1hot_lb[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2ce64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# let's now combine the numerical and categorical pipelines\n",
    "num_attribs = list(housing_num) # columns \n",
    "cat_attribs = [\"ocean_proximity\"]\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "        ('selector', DataFrameSelector(num_attribs)),\n",
    "        ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "        ('attribs_adder', CombinedAttributesAdder()),\n",
    "        ('std_scaler', StandardScaler()),\n",
    "    ])\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "        ('selector', DataFrameSelector(cat_attribs)),\n",
    "        ('label_binarizer', PipelineFriendlyLabelBinarizer()),\n",
    "    ])\n",
    "\n",
    "# and concatenate them with FeatureUnion class\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "full_pipeline = FeatureUnion(transformer_list=[\n",
    "        (\"num_pipeline\", num_pipeline),\n",
    "        (\"cat_pipeline\", cat_pipeline),\n",
    "    ])\n",
    "\n",
    "# this is the final transformation result!\n",
    "housing_prepared = full_pipeline.fit_transform(housing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c835cd59",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Pipeline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_1284\\1732955247.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;31m# (b)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m full_pipeline_with_predictor = Pipeline([\n\u001b[0m\u001b[0;32m     18\u001b[0m         \u001b[1;33m(\u001b[0m\u001b[1;34m\"preparation\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfull_pipeline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[1;33m(\u001b[0m\u001b[1;34m\"linear\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLinRegStatsmodels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Pipeline' is not defined"
     ]
    }
   ],
   "source": [
    "# (a)\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class LinRegStatsmodels():\n",
    "    def __init__(self):\n",
    "        self.models = []\n",
    "    def fit(self,X,y):\n",
    "        X = sm.add_constant(X)\n",
    "        model = sm.OLS(y,X)\n",
    "        res = model.fit()\n",
    "        self.models.append(res)\n",
    "    def predict(self,data): \n",
    "        return self.models[-1].predict(sm.add_constant(data,has_constant='add'))\n",
    "    \n",
    "# (b)\n",
    "full_pipeline_with_predictor = Pipeline([\n",
    "        (\"preparation\", full_pipeline),\n",
    "        (\"linear\", LinRegStatsmodels())\n",
    "    ])\n",
    "# A full pipeline with both preparation and prediction\n",
    "full_pipeline_with_predictor.fit(housing, housing_labels)\n",
    "#####################################################################\n",
    "# pay attention to the difference between fit_transform and transform\n",
    "# no need to fit on the test data\n",
    "some_data = housing.iloc[:5]\n",
    "processed_data = full_pipeline.transform(some_data)\n",
    "pred = full_pipeline_with_predictor[1].models[-1].predict(sm.add_constant(processed_data,has_constant='add'))\n",
    "print(full_pipeline_with_predictor.predict(some_data) == pred)\n",
    "\n",
    "residual = full_pipeline_with_predictor[1].models[-1].resid\n",
    "plt.scatter(range(len(residual)),residual,s=0.1)\n",
    "plt.ylabel(\"Residual\")\n",
    "plt.show()\n",
    "\n",
    "print(full_pipeline_with_predictor[1].models[-1].summary())\n",
    "print('###########################################################################################')\n",
    "print(full_pipeline_with_predictor[1].models[-1].get_robustcov_results().summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29dd0a7f",
   "metadata": {},
   "source": [
    "(c)\n",
    "Under OLS, we can see that with 95% confidence level, x4(total_rooms), x10(rooms_per_household),x13(dummy variable for the categorical variable ocean_proximity) are not statistically significant. \\\n",
    "Under OLS with robust standard error, with 95% confidence level,x4(total_rooms), x5(total_bedrooms), x10(rooms_per_household) are not statistically significant. \\\n",
    "Although the residual plot does not show any obvious pattern of heteroskedasticity, the test results actually changes under robust standard errors. That being said, there is heteroskedasticity in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "40947f1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[241862.50684923]\n"
     ]
    }
   ],
   "source": [
    "# (d)\n",
    "new_data = pd.DataFrame({\"longitude\":[-118.8],\"latitude\":[34.19],\"housing_median_age\":[4],\"total_rooms\":[15572],\"total_bedrooms\":[2222],\"population\":[5495],\n",
    "                      \"households\":[2152],\"median_income\":[housing[\"median_income\"].median()],\"ocean_proximity\":[\"<1H OCEAN\"]})\n",
    "print(full_pipeline_with_predictor.predict(new_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac01c4fb",
   "metadata": {},
   "source": [
    "Problems with the given data: \\\n",
    "(1) The data given does not have column named \"median_income\", so we suppose that the value is the median of the value of \"median_income\" in the original dataset. \\\n",
    "(2) The \"ocean_proximity\" give is \"1H OCEAN\", which not belongs to any class of the categorical variable \"ocean_proximity\". We only has \"<1H OCEAN\", so we assume that it is a typo and should be \"<1H OCEAN\" instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "d6447179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OLS C.I.  [106882.4895938003, 376842.52410466364]\n",
      "OLS + robust standard errors C.I.  [106179.44452046955, 377545.5691779944]\n"
     ]
    }
   ],
   "source": [
    "# (d)\n",
    "# 95% Confidence Interval for the predicted value:\n",
    "# OLSResults.get_prediction() method will return a linear_model.PredictionResults object\n",
    "# The prediction results instance contains prediction and prediction variance \n",
    "# and can on demand calculate confidence intervals and summary tables for the prediction of the mean and of new observations.\n",
    "# OLS:\n",
    "processed_new_data = full_pipeline.transform(new_data)\n",
    "pred_model = full_pipeline_with_predictor[1].models[-1].get_prediction(sm.add_constant(processed_new_data, has_constant='add'))\n",
    "confidence_interval = list(pred_model.conf_int(0.05)[0])\n",
    "print(\"OLS C.I. \", confidence_interval)\n",
    "robust_pred_model =  full_pipeline_with_predictor[1].models[-1].get_robustcov_results().get_prediction(sm.add_constant(processed_new_data, has_constant='add'))\n",
    "robust_confidence_interval = list(robust_pred_model.conf_int(0.05)[0])\n",
    "print(\"OLS + robust standard errors C.I. \", robust_confidence_interval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0abf78",
   "metadata": {},
   "source": [
    "(d)\\\n",
    "For the point estimator, OLS and OLS + robust standard errors produce the same estimated value. \\\n",
    "For the confidence interval, I would choose OLS + robust standard errors since it can relieve heteroskedasticity. In terms of model criteria, they have same $R^2$, AIC, BIC, etc. The only differences lie in statistical inference. The F statistic of robust OLS is much more larger than that of OLS."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c397a92",
   "metadata": {},
   "source": [
    "# Q2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "dd9b2836",
   "metadata": {},
   "outputs": [],
   "source": [
    "ff5_path = './datasets/F-F_Research_Data_5_Factors_2x3.csv'\n",
    "equityPrice_path = './datasets/QMNIX.csv'\n",
    "ff5 = pd.read_csv(ff5_path, dtype={'Month':str})\n",
    "ff5 = ff5.dropna()\n",
    "equityPrice = pd.read_csv(equityPrice_path,usecols=['Date', 'Adj Close']).rename(columns={'Adj Close':'AdjClose'})\n",
    "equityPrice.Date = equityPrice.Date.apply(lambda x:''.join(x.split('-')))\n",
    "equityPrice['Date'] = equityPrice.Date.apply(lambda x:x[:-2])\n",
    "equityPrice = equityPrice.dropna()\n",
    "months = equityPrice[\"Date\"].unique().tolist()\n",
    "Ret = {}\n",
    "for month in months:\n",
    "    if month in [months[0], months[-1]]:\n",
    "        continue\n",
    "    this_month = equityPrice[equityPrice[\"Date\"] == month]\n",
    "    preMonth = [i for i in months if int(i) < int(month)][-1]\n",
    "    pre_month = equityPrice[equityPrice[\"Date\"] == preMonth]\n",
    "    first = pre_month[\"AdjClose\"].iloc[-1]\n",
    "    last = this_month[\"AdjClose\"].iloc[-1]\n",
    "    Ret[month] = np.log(last/first)\n",
    "ff5['RET'] = ff5.Month.apply(lambda x:Ret[x] if x in Ret else np.nan) - ff5.RF/100\n",
    "ff5 = ff5[['Month','Mkt-RF','SMB','HML','RMW','CMA','RET']]\n",
    "ff5 = ff5.dropna().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "e44da66c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                    RET   R-squared:                       0.396\n",
      "Model:                            OLS   Adj. R-squared:                  0.366\n",
      "Method:                 Least Squares   F-statistic:                     12.98\n",
      "Date:                Sat, 30 Sep 2023   Prob (F-statistic):           1.02e-09\n",
      "Time:                        21:31:43   Log-Likelihood:                 252.11\n",
      "No. Observations:                 105   AIC:                            -492.2\n",
      "Df Residuals:                      99   BIC:                            -476.3\n",
      "Df Model:                           5                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          0.0028      0.002      1.217      0.226      -0.002       0.007\n",
      "Mkt-RF        -0.0009      0.001     -1.659      0.100      -0.002       0.000\n",
      "SMB           -0.0010      0.001     -1.002      0.319      -0.003       0.001\n",
      "HML            0.0033      0.001      3.895      0.000       0.002       0.005\n",
      "RMW            0.0022      0.001      1.790      0.077      -0.000       0.005\n",
      "CMA            0.0016      0.001      1.237      0.219      -0.001       0.004\n",
      "==============================================================================\n",
      "Omnibus:                        1.307   Durbin-Watson:                   1.832\n",
      "Prob(Omnibus):                  0.520   Jarque-Bera (JB):                0.838\n",
      "Skew:                          -0.181   Prob(JB):                        0.658\n",
      "Kurtosis:                       3.245   Cond. No.                         5.14\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "#####################################################################################\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                    RET   R-squared:                       0.396\n",
      "Model:                            OLS   Adj. R-squared:                  0.366\n",
      "Method:                 Least Squares   F-statistic:                     9.291\n",
      "Date:                Sat, 30 Sep 2023   Prob (F-statistic):           2.80e-07\n",
      "Time:                        21:31:43   Log-Likelihood:                 252.11\n",
      "No. Observations:                 105   AIC:                            -492.2\n",
      "Df Residuals:                      99   BIC:                            -476.3\n",
      "Df Model:                           5                                         \n",
      "Covariance Type:                  HC1                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          0.0028      0.002      1.169      0.245      -0.002       0.008\n",
      "Mkt-RF        -0.0009      0.001     -1.646      0.103      -0.002       0.000\n",
      "SMB           -0.0010      0.001     -0.882      0.380      -0.003       0.001\n",
      "HML            0.0033      0.001      3.645      0.000       0.002       0.005\n",
      "RMW            0.0022      0.001      1.804      0.074      -0.000       0.005\n",
      "CMA            0.0016      0.001      1.275      0.205      -0.001       0.004\n",
      "==============================================================================\n",
      "Omnibus:                        1.307   Durbin-Watson:                   1.832\n",
      "Prob(Omnibus):                  0.520   Jarque-Bera (JB):                0.838\n",
      "Skew:                          -0.181   Prob(JB):                        0.658\n",
      "Kurtosis:                       3.245   Cond. No.                         5.14\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors are heteroscedasticity robust (HC1)\n"
     ]
    }
   ],
   "source": [
    "# Regression\n",
    "# OLS\n",
    "import statsmodels.api as sm\n",
    "X = sm.add_constant(ff5[['Mkt-RF','SMB','HML','RMW','CMA']])\n",
    "model = sm.OLS(ff5['RET'], X)\n",
    "result = model.fit()\n",
    "print(result.summary())\n",
    "print(\"#####################################################################################\")\n",
    "# OLS + Robust standard errors\n",
    "print(result.get_robustcov_results().summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81bdd50",
   "metadata": {},
   "source": [
    "Again, we can find that two models have same values for criteria that have nothing to do with inference. \\\n",
    "The F and t statistics have not changed much under heteroskedastic standard errors, and still, only HML is statistically significant with 95% confidence level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3665eb14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
