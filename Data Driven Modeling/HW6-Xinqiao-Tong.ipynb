{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab990a3c",
   "metadata": {},
   "source": [
    "# Q1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0d7f10",
   "metadata": {},
   "source": [
    "(a) \n",
    "\n",
    "Let $\\hat{\\beta}$ denote Ridge estimator. Then MSE for predictions produced by ridge regression is:\n",
    "$$\n",
    "\\mathbb{E}[\\sum_{i=1}^{n}(\\hat{y} - y)^2] =\\mathbb{E}\\sum_{i=1}^{n}((X^T)_i^T\\hat{\\beta} -(X^T)_i^T\\beta-\\epsilon_i)^2.\n",
    "$$\n",
    "\n",
    "As  $\\hat{\\beta}=(X^TX+\\lambda I)^{-1}X^Ty=(X^TX+\\lambda I)^{-1}X^T(X\\beta+\\epsilon)$, the expectation of the ridge estimator $\\mathbb{E}[\\hat{\\beta}] = (X^TX+\\lambda I)^{-1}X^TX\\beta$. Hence the expected value of prediction is $\\mathbb{E}[X\\hat{\\beta}] = X\\mathbb{E}[\\hat{\\beta}]$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0276f26f",
   "metadata": {},
   "source": [
    "Since $\\epsilon \\sim N(0,\\sigma^2I)$,\n",
    "\\begin{equation*}\n",
    "\\begin{aligned}\n",
    "Bias &=\\mathbb{E}[\\sum_{i=1}^{n}( (X^T)_i^T\\mathbb{E}[\\hat{\\beta}]-(X^T)_i^T\\beta-\\epsilon_i)^2] \\\\&= \\sum_{i=1}^{n} \\mathbb{E}[(X^T)_i^T(\\mathbb{E}[\\hat{\\beta}]-\\beta)-\\epsilon_i]^2\\\\& = \\sum_{i=1}^{n} (\\mathbb{E}[\\hat{\\beta}]-\\beta)^{T}(X^T)_i(X^T)_i^T (\\mathbb{E}[\\hat{\\beta}]-\\beta) + n\\sigma^2 \\\\& =(\\mathbb{E}[\\hat{\\beta}]-\\beta)^{T}X^TX (\\mathbb{E}[\\hat{\\beta}]-\\beta) + n\\sigma^2 \\\\ Variance &= \\mathbb{E}[\\sum_{i=1}^{n}((X^T)_i^T(\\mathbb{E}[\\hat{\\beta}] -\\hat{\\beta}))^2] \n",
    "\\end{aligned}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8829d3ff",
   "metadata": {},
   "source": [
    "Let $X^TX = UDU^T$ be the eigenvalue decomposition of $X^TX$, where $U$ is a square matrix and $D = diag(\\sigma_1,\\dots,\\sigma_r)$ with $\\sigma_1\\geq\\dots \\geq \\sigma_r \\geq 0$, $X\\in\\mathbb{R}^{n\\times r}$.\n",
    "\n",
    "Since $\\mathbb{E}[\\hat{\\beta}] -\\beta = (X^TX+\\lambda I)^{-1} (X^TX+\\lambda I-\\lambda I)\\beta - \\beta =-\\lambda (X^TX+\\lambda I)^{-1} \\beta$,\n",
    "\n",
    "$Bias =  \\lambda ^{2}\\beta^{T}(X^TX+\\lambda I)^{-1}X^TX(X^TX+\\lambda I)^{-1}\\beta + n\\sigma^2$. Substitute $X^TX = UDU^T$,\n",
    "we have\n",
    "\n",
    "$Bias = \\beta^T UDU^T\\beta + n\\sigma^2$, where $ D = diag(\\frac{\\sigma_i}{(\\sigma_i/\\lambda+1)^2})$, $i=1,2,\\dots, r$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5ca487",
   "metadata": {},
   "source": [
    "For variance,\n",
    "\\begin{equation*}\n",
    "\\begin{aligned}\n",
    "Variance &= \\mathbb{E}[\\sum_{i=1}^{n}((\\mathbb{E}[\\hat{\\beta}] -\\hat{\\beta})^T(X^T)_i^T)^2 ]\\\\\n",
    "&=\\mathbb{E}[(\\mathbb{E}[\\hat{\\beta}] -\\hat{\\beta})^T X^TX (\\mathbb{E}[\\hat{\\beta}] -\\hat{\\beta})] \\quad as \\quad \\mathbb{E}[\\hat{\\beta}]-\\hat{\\beta}=-(X^TX+\\lambda I)^{-1}X^T\\epsilon \\\\\n",
    "&=  \\mathbb{E}[\\epsilon^TX(X^TX+\\lambda I)^{-1}X^TX(X^TX+\\lambda I)^{-1}X^T\\epsilon ] \\\\\n",
    "&= \\mathbb{E}[ tr(\\epsilon^TX(X^TX+\\lambda I)^{-1}X^TX(X^TX+\\lambda I)^{-1}X^T\\epsilon)] \\\\\n",
    "&= \\mathbb{E} [tr(\\epsilon\\epsilon^TX(X^TX+\\lambda I)^{-1}X^TX(X^TX+\\lambda I)^{-1}X^T)]\\\\\n",
    "&=tr (\\mathbb{E}[\\epsilon\\epsilon^T]X(X^TX+\\lambda I)^{-1}X^TX(X^TX+\\lambda I)^{-1}X^T) \\\\\n",
    "&=tr (\\sigma^2X^TX(X^TX+\\lambda I)^{-1}X^TX(X^TX+\\lambda I)^{-1})\n",
    "\\end{aligned}\n",
    "\\end{equation*}\n",
    "\n",
    "Plug in the eigendecompositions of $X^TX$ and $X^TX+\\lambda I$, we have\n",
    "\\begin{equation*}\n",
    "\\begin{aligned}\n",
    "Variance &= n\\sigma^2tr(U\\Sigma U^TU(\\Sigma+\\lambda I)^{-1}U^TU\\Sigma U^TU(\\Sigma+\\lambda I)^{-1}U^T) \\\\\n",
    "&= n\\sigma^2tr(\\Sigma(\\Sigma+\\lambda I)^{-1}\\Sigma(\\Sigma+\\lambda I)^{-1}) \\\\\n",
    "&= \\sum_{i=1}^{r} \\frac{\\sigma_i^2}{(\\sigma_i+\\lambda)^2}\n",
    "\\end{aligned}\n",
    "\\end{equation*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86be56d",
   "metadata": {},
   "source": [
    "Bias-Variance Decomposition:\n",
    "\\begin{equation*}\n",
    "\\begin{aligned}\n",
    "MSE &= \\mathbb{E}\\sum_{i=1}^{n}(\\hat{y} - y) \\\\\n",
    "&= \\mathbb{E}\\sum_{i=1}^{n}((X^T)_i^T\\hat{\\beta} -(X^T)_i^T\\beta-\\epsilon_i)^2\\\\\n",
    "&= \\mathbb{E} [\\sum_{i=1}^{n}((X^T)_i^T(\\hat{\\beta}- \\mathbb{E}[\\hat{\\beta}]) +(X^T)_i^T\\mathbb{E}[\\hat{\\beta}]-(X^T)_i^T\\beta-\\epsilon_i)^2]\\\\\n",
    "& = \\mathbb{E}[\\sum_{i=1}^{n}((X^T)_i^T(\\mathbb{E}[\\hat{\\beta}] -\\hat{\\beta}))^2] + \\mathbb{E}[\\sum_{i=1}^{n}( (X^T)_i^T\\mathbb{E}[\\hat{\\beta}]-(X^T)_i^T\\beta-\\epsilon_i)^2]\\\\ \n",
    "& = Variance + Bias\n",
    "\\end{aligned}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbe212e",
   "metadata": {},
   "source": [
    "(b)\n",
    "\n",
    "When  $\\lambda \\rightarrow 0$,\n",
    "\n",
    "$\\frac{\\sigma_i}{(\\sigma_i/\\lambda+1)^2}\\rightarrow 0 $, so bias will go to zero;\n",
    "\n",
    "$\\frac{\\sigma_i^2}{(\\sigma_i+\\lambda)^2} \\rightarrow 1$, so variance will go to $r$.\n",
    "\n",
    "When  $\\lambda \\rightarrow \\infty$,\n",
    "\n",
    "$\\frac{\\sigma_i}{(\\sigma_i/\\lambda+1)^2}\\rightarrow \\sigma_i $, bias will become large,\n",
    "\n",
    "$\\frac{\\sigma_i^2}{(\\sigma_i+\\lambda)^2} \\rightarrow 0$, so variance will go to 0.\n",
    "\n",
    "\n",
    "Since variance is monotonically decreasing while bias is monotonically increasing, we can definitely find some $\\lambda$ that minimizes MSE. But in practice we cannot achieve this since we do not know the true coefficient, $\\beta$. We would turn to techniques like Cross Validation to select the best $\\lambda$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24998987",
   "metadata": {},
   "source": [
    "# Q2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c743a5e5",
   "metadata": {},
   "source": [
    "(a) The $L_1$ minimizer is $\\mathrm{Median}[y|\\textbf{x}]$. \n",
    "\\begin{equation*}\n",
    "\\mathrm{E}[|y-\\alpha| \\mid \\textbf{x}]  =f(\\alpha)= \\int_{-\\infty}^\\alpha (\\alpha-y)p_{y|\\textbf{x}}(y)dy + \\int_\\alpha^{\\infty} (y - \\alpha)p_{y|\\textbf{x}}(y)dy.\n",
    "\\end{equation*}\n",
    "Then we have \n",
    "\\begin{equation*}\n",
    "f'(\\alpha) = \\alpha p_{y|\\textbf{x}}(\\alpha) + \\mathrm{P}(y<\\alpha) - \\alpha p_{y|\\textbf{x}}(\\alpha)- \\alpha p_{y|\\textbf{x}}(\\alpha) - \\mathrm{P}(y>\\alpha) + \\alpha p_{y|\\textbf{x}}(\\alpha) = 2\\mathbb{P}(y<\\alpha)-1 =0.\n",
    "\\end{equation*}\n",
    "Therefore, we can solve $\\alpha^* = \\mathrm{Median}[y|\\textbf{x}]$. Check the second order derivative, $f''(\\alpha^*) = 2p_{y|\\textbf{x}}(\\alpha^*)>0$, therefore, the $L_1$ loss is minimized when $\\hat{y} = \\alpha^*$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb9b7fd",
   "metadata": {},
   "source": [
    "(b) If we long an European call and an European put option, if we choose $K= \\mathrm{Median}[S_T|S_t]$, the expected payoff of our portfolio is minimized. That is, $\\mathbb{E}[|S_T-K| \\mid S_t] $ is minimized when $K= \\mathrm{Median}[S_T|S_t]$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5255fa",
   "metadata": {},
   "source": [
    "# Q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7d58c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (a)\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "\n",
    "ff5_path = './datasets/F-F_Research_Data_5_Factors_2x3.csv'\n",
    "equityPrice_path = './datasets/QMNIX.csv'\n",
    "ff5 = pd.read_csv(ff5_path, dtype={'Month':str})\n",
    "ff5 = ff5.dropna()\n",
    "equityPrice = pd.read_csv(equityPrice_path,usecols=['Date', 'Adj Close'])/\n",
    "                                    .rename(columns={'Adj Close':'AdjClose'})\n",
    "equityPrice.Date = equityPrice.Date.apply(lambda x:''.join(x.split('-')))\n",
    "equityPrice['Date'] = equityPrice.Date.apply(lambda x:x[:-2])\n",
    "equityPrice = equityPrice.dropna()\n",
    "months = equityPrice[\"Date\"].unique().tolist()\n",
    "Ret = {}\n",
    "for month in months:\n",
    "    if month in [months[0], months[-1]]:\n",
    "        continue\n",
    "    this_month = equityPrice[equityPrice[\"Date\"] == month]\n",
    "    preMonth = [i for i in months if int(i) < int(month)][-1]\n",
    "    pre_month = equityPrice[equityPrice[\"Date\"] == preMonth]\n",
    "    first = pre_month[\"AdjClose\"].iloc[-1]\n",
    "    last = this_month[\"AdjClose\"].iloc[-1]\n",
    "    Ret[month] = np.log(last/first)\n",
    "ff5['RET'] = ff5.Month.apply(lambda x:Ret[x] if x in Ret else np.nan) - ff5.RF/100\n",
    "ff5 = ff5[['Mkt-RF','SMB','HML','RMW','CMA', 'RET']]\n",
    "ff5 = ff5.dropna().reset_index(drop=True)\n",
    "factor_data = ff5[['Mkt-RF','SMB','HML','RMW','CMA']]\n",
    "labels = ff5['RET']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe48f5e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OLS MSE:  0.00049420\n"
     ]
    }
   ],
   "source": [
    "# OLS\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "X_train, X_test, y_train, y_test = train_test_split(factor_data,labels, \n",
    "                                                train_size = .8, random_state=1)\n",
    "pipeline_1 = Pipeline([\n",
    "        ('std_scaler', StandardScaler()),\n",
    "        ('OLS', LinearRegression())\n",
    "    ])\n",
    "pipeline_1.fit(X_train, y_train)\n",
    "print('OLS MSE: ', \"%.8f\" %  mean_squared_error(y_train, pipeline_1.predict(X_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f38532b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elastic Net MSE:  0.00049420\n"
     ]
    }
   ],
   "source": [
    "# Elastic Net\n",
    "from sklearn.linear_model import ElasticNet\n",
    "pipeline_2 = Pipeline([\n",
    "        ('std_scaler', StandardScaler()),\n",
    "        ('Elastic_Net', ElasticNet(alpha=1e-5, random_state=0))\n",
    "    ])\n",
    "pipeline_2.fit(X_train, y_train)\n",
    "print('Elastic Net MSE: ', \"%.8f\" %  mean_squared_error(y_train, pipeline_2.predict(X_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c8a90b",
   "metadata": {},
   "source": [
    "(a) We can see that the estimations in Elastic Net are approximately the same as those in OLS, and there MSE is also almost the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7ca48b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (b)\n",
    "# (i)\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "# modify our pipeline by including artificial features\n",
    "# add 15 new features, namely, product of any two features\n",
    "class add_artificial(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        cur_features = list(X.columns)\n",
    "        cur_num = len(cur_features)\n",
    "        for i in range(cur_num):\n",
    "            for j in range(i,cur_num):\n",
    "                ind = int((10-i+1)*i/2 +j-i+1)\n",
    "                X[\"Artificial\" + str(ind)] = X[cur_features[i]].multiply(X[cur_features[j]])\n",
    "        return X\n",
    "\n",
    "pipeline_3 = Pipeline([\n",
    "        ('add_features', add_artificial()),\n",
    "        ('std_scaler', StandardScaler()),\n",
    "    ])\n",
    "added_ff5 = pipeline_3.fit_transform(factor_data)\n",
    "# print(added_ff5.shape) (105, 20)\n",
    "X_train, X_test, y_train, y_test = train_test_split(added_ff5,labels, \n",
    "                                                    train_size = .8, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dfa338b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elastic Net MSE with more features:  0.00036355\n",
      "OLS MSE with more features:  0.00036354\n"
     ]
    }
   ],
   "source": [
    "EN_added = ElasticNet(alpha=1e-5, random_state=0)\n",
    "EN_added.fit(X_train, y_train)\n",
    "EN_added.coef_\n",
    "print('Elastic Net MSE with more features: ', \"%.8f\" %  mean_squared_error(y_train, EN_added.predict(X_train)))\n",
    "lin_added = LinearRegression()\n",
    "lin_res = lin_added.fit(X_train, y_train)\n",
    "print('OLS MSE with more features: ', \"%.8f\" %  mean_squared_error(y_train, lin_res.predict(X_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743a9ad1",
   "metadata": {},
   "source": [
    "(b) (i) Actually, the MSE has been reduced with new artificial features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c331e0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00103788 0.00019447 0.00021237 0.00064269 0.00126255 0.0017169\n",
      " 0.00121753 0.00111186 0.00565066 0.00110508]\n",
      "[0.00103696 0.00019452 0.00020992 0.00064022 0.00125865 0.0016882\n",
      " 0.00119713 0.00110377 0.00559111 0.00109904]\n",
      "[ True False  True  True  True  True  True  True  True  True]\n"
     ]
    }
   ],
   "source": [
    "# (ii)\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "score_lin = -cross_val_score(\n",
    "            lin_added, added_ff5, labels, \n",
    "            scoring=\"neg_mean_squared_error\", # for some reason cross_val_score computes negative of MSE\n",
    "            cv=10)\n",
    "\n",
    "score_ElasticNet = -cross_val_score(\n",
    "            EN_added, added_ff5, labels, \n",
    "            scoring=\"neg_mean_squared_error\", # for some reason cross_val_score computes negative of MSE\n",
    "            cv=10)\n",
    "\n",
    "print(score_lin)\n",
    "print(score_ElasticNet)\n",
    "print(score_ElasticNet < score_lin)\n",
    "# We can see that generally Elastic Net outperforms OLS in cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b76ad1",
   "metadata": {},
   "source": [
    "(iii) No. The Bias-Variance tradeoff:\n",
    "\\begin{equation*}\n",
    "\\begin{aligned}\n",
    "E[(y-\\hat{y})^2] = & (E[\\hat{y}]-E[y])^2 + E[(\\hat{y}-E[\\hat{y}] )^2]\\\\\n",
    "= & Bias + Variance\n",
    "\\end{aligned}\n",
    "\\end{equation*}\n",
    "The MSE can therefore come from both bias and from variance of the estimator, but we cannot reduce them at the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3b60e333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 train MSE: \n",
      " [(0, 0.00036353913757446264), (1, 0.0003635490713338929), (2, 0.00036357552972293597), (3, 0.0003636184757236287), (4, 0.00036367779673814656)]\n",
      "Top 5 test MSE: \n",
      " [(269, 0.0005796747348492464), (268, 0.0005796845878698204), (270, 0.0005799117762159533), (267, 0.0005801419864872533), (271, 0.000580149914625285)]\n"
     ]
    }
   ],
   "source": [
    "# (iv)\n",
    "train_mse = []\n",
    "test_mse = []\n",
    "alpha_list = np.linspace(1e-6,1e-2,1000)\n",
    "for i in alpha_list:\n",
    "    EN_added = ElasticNet(alpha=i)\n",
    "    EN_added.fit(X_train, y_train)\n",
    "    train_mse.append(mean_squared_error(y_train, EN_added.predict(X_train)))\n",
    "    test_mse.append(mean_squared_error(y_test, EN_added.predict(X_test)))\n",
    "\n",
    "ranked_train = sorted(enumerate(train_mse),key=lambda x:x[1])[:5]\n",
    "ranked_test = sorted(enumerate(test_mse),key=lambda x:x[1])[:5]\n",
    "print(\"Top 5 train MSE: \\n\", ranked_train)\n",
    "print(\"Top 5 test MSE: \\n\", ranked_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99030b8f",
   "metadata": {},
   "source": [
    "(iv) We can see clearly that the best model in terms of MSE in the training set is not the best one in the test model. Possibly because that when alpha is very small, Elastic Net performs just like OLS and there is little regularization. Then the model has little bias but more variance in its MSE, despite that it has minimum MSE. It is kind of overfitted in some sense and hence cannot perform well in the test set."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
