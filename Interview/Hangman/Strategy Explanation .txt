Hope my comments in the notebook has already helped you understand my logic. 
In a nutshell, my strategy focus on key elements in a word, namely vowels and root/affix and make inferences based on them.
Firstly we discovered that 90% percentile of percentage of vowels in the training set is 0.5, then it is not a good decsion to continue guessing vowels when percentage of vowels in a word is larger than 0.55.

In our main function guess(), firstly we do as the baseline to get the current_dictionary, whose items satisfy re.match(item, clean_word).
1. In order to guess starting from a nearly blank word, we guess the letter that appears in the most words in current dictionary. This is achieved by the 'times_in_words' function. We sort these candidate letters according to number of words they appears in and guess these letters iteratively until we run out of them or we have successfully guesses 40% of the word, since if we have already guessed part of the word, then we had better move on to search in a more logical sense since we already have some pattern to infer. 
2. Now we probably have already initiated our word (guessed part of it). we make a root library from the whole training set, it is a dict and has the form  {n (length of root) : Counter(list of root of length n) for n in range(3, 30)}. For example, (roots of length) 4: tion, tive, ness... It stores all substrings appears in word in the training set, stored by their length in a dict as a form of Counter.
Then what we do? For the target word (with form like '.pp.e'), we look for substring s in root library such that len(s) = len(target) and re.match(target,s) = True. We add s to a list called temp counting multiplicity, then the more times a certain letter appears in word in temp, (can achieve this by times_in_words function), the more likely it is to fit in the target word. This is what we do in 'raw_scan' function, which returns a sorted Counter, and the 0th letter in Counter is the most likely one to fit in the target word and should be our guess.
3. Then we turn to search locally, create a root lib from the current dictionary, for roots with 3 <= length < 9. Then we compare all substrings in clean_word with roots from current dictionary. If there is a match, we add possible choices of letters in that root into a list. Then our guess is the letter that appears the most times in that list.
4. Then we want to step out from local patterns and search globally in the learning set for roots of 3 <= length < 9. The logic here is similar to the previous one, other than that here we compare substrings in clean_word with roots from the whole training set.
5. Finally if we run out of choices we do as what the baseline do in the end, pick letters that appears most times in the whole training set in order.

This is just a brief intro to my strategy, for more detailed infomation please see the comments in my code. Thank you for your consideration and look forward to joining your team as a Global Alpha Researcher.